apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
  name: spark-tekton-demo-run
spec:
  inputs:
    params:
      - name: SPARK_CLUSTER_NAME
        type: string
        description: |
          The name of the spark cluster to use.
          Spark connection url expected to be of the form: spark://${SPARK_CLUSTER_NAME}:7077
      - name: CONTEXT
        type: string
        description: The directory that contains your spark app and its resources.
        default: '.'
      - name: SPARK_APP
        type: string
        description: The name of a python file in CONTEXT dir that runs your spark job, for example 'spark-pi.py'
  workspaces:
    - name: staged
      # your spark application repository is expected to be loaded onto /staged/src
      readOnly: true
      mountPath: /staged
  results:
    - name: task-pod-name
      description: the name of the pod (and sevice) created for this Task
  steps:
    # There is a very strange bug where if I try to add more than one step to this task,
    # the companion service I create below stops resolving, and the spark job does not work.
    # It seems to have something to do with having a pod that has stopped containers in it
    # (in other words, already-completed steps in the task). I would guess that you could
    # add steps *after*, since at that point the companion service is no longer needed.
    - name: run-spark-job
      image: quay.io/erikerlandson/spark-tekton-demo-s2i:latest
      env:
        - name: SPARK_CLUSTER
          value: $(inputs.params.SPARK_CLUSTER_NAME)
        - name: TASK_RUN_NAME
          value: $(context.taskRun.name)
        - name: TASK_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
      script: |
        # return the task result
        echo -n $TASK_POD_NAME > $(results.task-pod-name.path)
        # A companion service is used by Spark executors to resolve directly
        # back to the pod running the driver (here, that is this task pod)
        # NOTE: I am cleaning up this Service object in a later task,
        # which could be a 'finally' task in latest versions of tekton
        cat > /tmp/companion-service.yaml <<- EOF
        kind: Service
        apiVersion: v1
        metadata:
          name: $TASK_POD_NAME
        spec:
          selector:
            tekton.dev/taskRun: $TASK_RUN_NAME
          clusterIP: None
          type: ClusterIP
          sessionAffinity: None
        EOF
        # create the companion service and wait for a few seconds to make sure it exists.
        /opt/pyspark-s2i/bin/oc apply -f /tmp/companion-service.yaml
        sleep 15
        # activate the pyspark virtualenv we defined via pipenv
        # this emulates entering 'pipenv shell', but we can stay in current shell.
        cd /opt/pyspark
        . $(pipenv --venv)/bin/activate
        # the pipenv shell normally enables these as well
        export PYTHONDONTWRITEBYTECODE=1
        export PIPENV_ACTIVE=1
        # now we can run our spark job from the context directory
        cd /staged/src/$(inputs.params.CONTEXT)
        python3 $(inputs.params.SPARK_APP)
