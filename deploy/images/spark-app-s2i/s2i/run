#!/bin/bash

echo "s2i run script"

# In the run mode we will create an ephemeral cluster, if asked for (otherwise, we assume
# some pre-existing cluster to connect to).
# Once we have a spark cluster, we run the spark-app.py file supplied by the user

# determine the spark name
export IS_EPHEMERAL_SPARK="no"
if [ "$S2I_SPARK_CLUSTER" == "ephemeral" ]; then
    SPARK_UUID=$(uuidgen | awk 'BEGIN { FS = "-" } ; { print $1 }')
    export S2I_SPARK_CLUSTER="spark-cluster-${SPARK_UUID}"
    echo
    echo "EPHEMERAL CLUSTER NAME: ${S2I_SPARK_CLUSTER}"
    export IS_EPHEMERAL_SPARK="yes"
fi

if [ "$IS_EPHEMERAL_SPARK" == "yes" ]; then
    echo
    echo "STANDING UP EPHEMERAL CLUSTER ${S2I_SPARK_CLUSTER}"
    cat > spark-cluster.yaml <<- EOF
kind: ConfigMap
apiVersion: v1
metadata:
  name: $S2I_SPARK_CLUSTER
  labels:
    radanalytics.io/kind: SparkCluster
data:
  config: >-
    worker:
      instances: 2
      cpuLimit: 1
      memoryLimit: 4Gi
      cpuRequest: 250m
      memoryRequest: 1Gi
    master:
      instances: 1
      cpuLimit: 1
      memoryLimit: 4Gi
      cpuRequest: 250m
      memoryRequest: 1Gi
    customImage: spark-app-s2i-spark:latest
    env:
    - name: SPARK_METRICS_ON
      value: prometheus
EOF
    echo
    echo "GENERATED spark cluster object YAML"
    echo ===========================
    cat spark-cluster.yaml
    echo ===========================

    echo
    echo "CREATING spark cluster configmap"
    /opt/spark-app/bin/oc login --token $(cat /run/secrets/kubernetes.io/serviceaccount/token) \
                      --certificate-authority=/run/secrets/kubernetes.io/serviceaccount/ca.crt \
                      https://kubernetes.default:443
    /opt/spark-app/bin/oc apply -f spark-cluster.yaml

    echo
    echo "WAITING for spark cluster availability"
    # make sure the spark operator has had time to see the new configmap and stand up a cluster
    sleep 240
fi


# This is the env var seen in the jupyter notebooks by convention, so set that to make a typical
# spark connection in a data scientist notebook see what it is expecting.
export SPARK_CLUSTER=$S2I_SPARK_CLUSTER

echo "RUNNING spark application:"
python3 /opt/spark-app/src/spark-app.py

if [ "$IS_EPHEMERAL_SPARK" == "yes" ]; then
    echo
    echo "TEARING DOWN EPHEMERAL SPARK ${S2I_SPARK_CLUSTER}"
    /opt/spark-app/bin/oc delete cm/${S2I_SPARK_CLUSTER}
fi

echo
echo "RUN FINISHED"
